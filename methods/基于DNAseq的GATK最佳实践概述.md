# 基于DNAseq的GATK最佳实践

原文链接: [ Best Practices for Variant Discovery in DNAseq](https://software.broadinstitute.org/gatk/documentation/article?id=3238)

这里只是讲解了步骤，概述，真正的实践不在这里。

**This article is part of the Best Practices documentation. See http://www.broadinstitute.org/gatk/guide/best-practices for the full documentation set.**

This is our recommended workflow for calling variants in DNAseq data from cohorts of samples, in which steps from data processing up to variant calling are performed per-sample, and subsequent steps are performed jointly on all the individuals in the cohort.

[![img](https://us.v-cdn.net/5019796/uploads/FileUpload/eb/44f317f8850ba74b64ba47b02d1bae.png)](https://us.v-cdn.net/5019796/uploads/FileUpload/eb/44f317f8850ba74b64ba47b02d1bae.png)

The workflow is divided in three main sections that are meant to be performed sequentially:

- Pre-processing: from raw DNAseq sequence reads (FASTQ files) to analysis-ready reads (BAM files)
- Variant discovery: from reads (BAM files) to variants (VCF files)
- Refinement and evaluation: genotype refinement, functional annotation and callset QC

------

### Pre-Processing

The data generated by the sequencers are put through some pre-processing steps to make it suitable for variant calling analysis. The steps involved are: Mapping and Marking Duplicates; Local Realignment Around Indels; and Base Quality Score Recalibration (BQSR); performed in that order.

#### Mapping and Marking Duplicates

The sequence reads are first mapped to the reference using BWA mem to produce a file in SAM/BAM format sorted by coordinate. The next step is to mark duplicates. The rationale here is that during the sequencing process, the same DNA molecules can be sequenced several times. The resulting duplicate reads are not informative and should not be counted as additional evidence for or against a putative variant. The duplicate marking process identifies these reads as such so that the GATK tools know they should ignore them.

#### Realignment Around Indels

Next, local realignment is performed around indels, because the algorithms that are used in the initial mapping step tend to produce various types of artifacts. For example, reads that align on the edges of indels often get mapped with mismatching bases that might look like evidence for SNPs, but are actually mapping artifacts. The realignment process identifies the most consistent placement of the reads relative to the indel in order to clean up these artifacts. It occurs in two steps: first the program identifies intervals that need to be realigned, then in the second step it determines the optimal consensus sequence and performs the actual realignment of reads.

#### Base Quality Score Recalibration

Finally, base quality scores are recalibrated, because the variant calling algorithms rely heavily on the quality scores assigned to the individual base calls in each sequence read. These scores are per-base estimates of error emitted by the sequencing machines. Unfortunately the scores produced by the machines are subject to various sources of systematic error, leading to over- or under-estimated base quality scores in the data. Base quality score recalibration is a process in which we apply machine learning to model these errors empirically and adjust the quality scores accordingly. This yields more accurate base qualities, which in turn improves the accuracy of the variant calls. The base recalibration process involves two key steps: first the program builds a model of covariation based on the data and a set of known variants, then it adjusts the base quality scores in the data based on the model.

------

### Variant Discovery

Once the data has been pre-processed as described above, it is put through the variant discovery process, i.e. the identification of sites where the data displays variation relative to the reference genome, and calculation of genotypes for each sample at that site. Because some of the variation observed is caused by mapping and sequencing artifacts, the greatest challenge here is to balance the need for sensitivity (to minimize false negatives, i.e. failing to identify real variants) vs. specificity (to minimize false positives, i.e. failing to reject artifacts). It is very difficult to reconcile these objectives in a single step, so instead the variant discovery process is decomposed into separate steps: variant calling (performed per-sample), joint genotyping (performed per-cohort) and variant filtering (also performed per-cohort). The first two steps are designed to maximize sensitivity, while the filtering step aims to deliver a level of specificity that can be customized for each project.

#### Per-Sample Variant Calling

We perform variant calling by running the HaplotypeCaller on each sample BAM file (if a sample's data is spread over more than one BAM, then pass them all in together) to create single-sample gVCFs. If there are more than a few hundred samples, we combine the gVCFs in batches of ~200 gVCFs using a specialized tool, CombineGVCFs. This will make the next step more tractable and reflects that the processing bottleneck lies with the number of input files and not the number of samples in those files.

#### Joint Genotyping

All available samples are then jointly genotyped by taking the gVCFs produced earlier and running GenotypeGVCFs on all of them together to create a set of raw SNP and indel calls. This cohort-wide analysis empowers sensitive detection of variants even at difficult sites.

#### Variant Quality Score Recalibration

Variant recalibration involves using a machine learning method to assign a well-calibrated probability to each variant call in a raw call set. We can then use this variant quality score in the second step to filter the raw call set, thus producing a subset of calls with our desired level of quality, fine-tuned to balance specificity and sensitivity.

------

### Refinement and evaluation

In this last section, we perform some refinement steps on the genotype calls (GQ estimation and transmission phasing), add functional annotations if desired, and do some quality evaluation by comparing the callset to known resources. None of these steps are absolutely required, and the workflow may need to be adapted quite a bit to each project's requirements.

------

**Important note on GATK versions**

 

The [Best Practices](http://www.broadinstitute.org/gatk/guide/best-practices) have been updated for GATK version 3. If you are running an older version, you should seriously consider upgrading. For more details about what has changed in each version, please see the [Version History](http://www.broadinstitute.org/gatk/guide/version-history) section. If you cannot upgrade your version of GATK for any reason, please look up the corresponding version of the GuideBook PDF (also in the [Version History](http://www.broadinstitute.org/gatk/guide/version-history) section) to ensure that you are using the appropriate recommendations for your version.


# [GATK最佳实践-基于全基因组和外显子测序检测Germline SNP & INDEL](http://rogerdudler.github.io/git-guide/index.zh.html)

## 一、分析流程图

​	基于全基因组或者外显子测序数据检测Germline SNP & INDEL, GATK官方推荐使用最佳实践(Best Practices)，分析流程如下图所示，主要分为三个部分：预处理（PRE-PROCESSING）、发现变异(VARIANT DISCOVERY)和优化变异集(CALLSET REFINEMENT)。

![img](https://software.broadinstitute.org/gatk/img/BP_workflow_3.6.png)

## 二、预处理(PRE-PROCESSING)

​	预处理过程是整理分析流程的第一步，是非常必须的。开始于FASTQ或者其他非BAM格式的文件，结束于用于第二步分析前的准备好的BAM文件。

​	大多数情况下，我们得到的测序数据为FASTQ格式，对于一个样品来说可能有一个或多个FASTQ文件，这样的数据是不能直接用于变异检测分析的。即使得到的是BAM格式的文件,为了最大化的技术正确性，你可能仍然需要做一些数据处理。

​	首先需要将测序的数据比对到参考基因组上，产生SAM/BAM格式的文件；其次标记重复reads 减少偏好性，这些重复的reads可能来源于PCR实验扩增；最后我们较正碱基的质量值，因为变异检测的算法非常依赖于每一个read中每一个碱基的质量分值。

### 1.比对参考基因组(Map to Reference)与标记重复(Mark Duplicates)

​	第一步我们首先将测序的reads比对到参考基因组上，产生SAM/BAM文件。对于DNA测序数据我们一般推荐BAW MEM，这个依赖于你的数据以及测序平台，你可以选择其他的比对工具。完成比对后，你需要确认比对结果是经过排序的。在这一过程中，你可以加入Read group的信息，也可以通过Picard AddOrReplaceReadGroups增加或者修改。

​	完成基因组比对后，接下来是标记重复，在测序的过程中，相同的DNA片段可能被测到多次，产生的重复reads没有更多的信息，也不能用于作为支持变异信息的证据，标记重复的过程，不是将reads过滤，而只是在检测出来重复的read，并加上一个重复标签（TAG）的信息。多数的GATK工具默认情况下是不使用这些reads信息。

​	这此过程中用到一些工具如BWA ,Picard 不属于GATK，所以我们不提供这些软件的详细文档，如果要了解更多，可以查看官网的文档页面。

​	下面我们将介绍一步步介绍如何比对和标记。

​	首先要保证测序数据是过滤了接头信息。（测序数据最好是经过质量过滤-译者）

​	1.确定read分组信息

​	read分组是下游GATK的关键信息，如果没有read的分组信息，GATK将不能正常使用，确保加入你所知道的read分组信息，更多信息可以查看SAM格式说明。

如下格式：

```
@RG\tID:group1\tSM:sample1\tPL:illumina\tLB:lib1\tPU:unit1 
```

每一个元素之间使用“\t”分隔。

​	2.生成read比对结果SAM文件

​	使用BWA,命令行如下：

```
bwa mem -M -R ’<read group info>’ -p reference.fa raw_reads.fq > aligned_reads.sam 
```

‘<read group info>' 即为1中定义的read分组信息，-M参数可以将BAM比对的short split 比对结果作为第二个比对信息。

​	结果文件为 aligned_reads.sam，包含了输入文件的信息，基因组的比对信息。我们使用的命令行是将Pairend fastq，合并在一个文件中，在比对其他格式的文件中需要适当调整命令行，具体可以阅读BWA 的使用文档。

​	3.转为BAM，排序和标记重复

​	这些预处理的操作过程是为了使数据适应GATK 工具。

​	使用Picard将文件从SAM转为排序后BAM格式。

```
java -jar picard.jar SortSam \ 
    INPUT=aligned_reads.sam \ 
    OUTPUT=sorted_reads.bam \ 
    SORT_ORDER=coordinate 
```

​	结果文件：sorted_reads.bam，此文件是根据位置坐标进行了排序，BAM文件是压缩后的文件，文件大小较SAM许多。

​	使用Picard标记重复，命令如下：

```
java -jar picard.jar MarkDuplicates \ 
    INPUT=sorted_reads.bam \ 
    OUTPUT=dedup_reads.bam \
    METRICS_FILE=metrics.txt
```

​	结果文件：dedup_reads.bam，这个文件格式同输入文件，只是标记了重复的reads。同时产生一个统计文件mertics.txt。

​	使用Picard对标记重复的BAM文件建立索引，命令如下：

```
java -jar picard.jar BuildBamIndex \ 
    INPUT=dedup_reads.bam 
```

​	结果文件:dup_reads.bam.bai

